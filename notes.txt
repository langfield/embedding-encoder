Assorted notes from the development process.

# ISSUE: Description of purpose. 
    
    The ideas is that we pick one (ora  few, this is "batch\_size"), and compute the distance from this embedding to all others, and train on this at each step. 
    A placeholder is a stand-in for our dataset. We'll assign data to it at a later date. Data is "fed" into the persistent TensorFlow network graph through these placeholders. 

# ISSUE: Multiprocessing hangs.  
    
    This is most likely because of this documented quirk of multiprocessing.Queue:

    Bear in mind that a process that has put items in a queue will wait before terminating until all the
    buffered items are fed by the “feeder” thread to the underlying pipe. (The child process can call the
    cancel_join_thread() method of the queue to avoid this behaviour.)

    This means that whenever you use a queue you need to make sure that all items which have been
    put on the queue will eventually be removed before the process is joined. Otherwise you cannot be
    sure that processes which have put items on the queue will terminate. Remember also that
    non-daemonic processes will be joined automatically.

    Basically, you need to make sure you get() all the items from a Queue to guarantee that all the
    processes which put something into that Queue will be able to exit.

# ISSUE: Deciding which dot product method to use for fastest execution. 

    The matmul version seems to take about 44s for 3 batches of size 10. 

    batch_slice = tf.slice(entire_embedding,slice_begin,slice_size)
    dist_row = sess.run(tf.matmul(batch_slice,emb_transpose)) # dot product

    Maybe if I sess.run the slice operation, then we might get better running time. 







    The tensordot version seems to take about 39s for 3 batches of size 10. 

    dist_row = sess.run(tf.tensordot(tf.slice(entire_embedding,slice_begin,slice_size),
    entire_embedding,[[1],[1]])) # dot product






    The matmul version seems to take about 30s for 3 batches of size 10.

    dist_row = sess.run(tf.matmul(tf.slice(entire_embedding,slice_begin,slice_size),emb_transpose)) # dot product

    FAST AF
